{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">异步流及 CUDA C/C++ 应用程序的可视化性能分析</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA工具包附带了 **Nsight Systems**，这是一个功能强大的GUI应用程序，可支持CUDA应用程序的开发。 Nsight Systems为被加速的应用程序生成图形化的活动时间表，其中包含有关CUDA API调用、内核执行、内存活动以及**CUDA流**的使用的详细信息。\n",
    "\n",
    "在本实验中，您将使用Nsight Systems时间表来指导您优化被加速的应用程序。此外，您还将学习一些中级的CUDA编程技术来支持您的工作：**手动的内存分配和迁移**； **固定**或**页面锁定**的主机内存；以及**非默认并发CUDA流**。\n",
    "\n",
    "本实验学习课程结束时，我们将为您提供一份评估测试，让您加速和优化一款简单的 n-body 模拟器，这可让您借此展示在本课程学习期间所掌握的技能。若您能够在保持正确性的同时加速模拟器，我们将为您颁发证书以资证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 预备知识\n",
    "\n",
    "为了充分利用本实验，您应该已经能够：\n",
    "\n",
    "- 编写，编译和运行既调用CPU函数又启动GPU核函数的C / C ++程序。\n",
    "- 使用执行配置(execution configuration)控制并行线程的层次结构。\n",
    "- 重构串行循环以在GPU上并行执行其迭代。\n",
    "- 分配和释放CUDA统一内存。\n",
    "- 了解统一内存在页面错误和数据迁移方面的行为。\n",
    "- 使用异步内存预取可以减少页面错误和数据迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标\n",
    "\n",
    "在完成本练习后，您将能够：\n",
    "\n",
    "- 使用**Nsight Systems**直观地描述由GPU加速的CUDA应用程序的时间表。\n",
    "- 使用**Nsight Systems**识别和利用CUDA应用程序中的优化机会。\n",
    "- 利用CUDA流在被加速的应用程序中并发执行核函数。\n",
    "- （ **可选的进阶内容** ）使用手动的设备内存分配，包括分配固定的内存，以便在并发CUDA流之间异步传输数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 运行Nsight Systems\n",
    "\n",
    "在此交互式实验环境中，我们设置了一个远程桌面，您可以从您的浏览器访问该桌面，并在其中启动和使用Nsight Systems。\n",
    "\n",
    "首先，为一个已经存在的向量加法程序创建一个性能报告文件，然后逐步执行一系列步骤，以在Nsight Systems中打开该报告文件，并提供良好的视觉体验。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成报告文件\n",
    "\n",
    "[`01-vector-add.cu`](01-vector-add.cu)（<--单击这个链接以在浏览器中对源文件进行编辑）包含一个正常的矢量加法程序。 使用下方的代码执行单元（通过按CTRL+ENTER，您可以执行它以及本实验中的任何代码执行单元）来编译并运行它。 您应该看到一条表明已成功完成的消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-no-prefetch 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，使用`nsys profile --stats = true`创建一个报告文件，您将可以在Nsight Systems可视化分析器中打开该文件。 在这里，我们使用-o标志为报告文件起一个容易记住的文件名："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-no-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-no-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-no-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t9913 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-no-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-no-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 9870 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-no-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   63.0       249804839           3      83268279.7           19879       249715643  cudaMallocManaged                                                               \n",
      "   32.1       127182152           1     127182152.0       127182152       127182152  cudaDeviceSynchronize                                                           \n",
      "    4.8        19200536           3       6400178.7         5596562         7852215  cudaFree                                                                        \n",
      "    0.0           53020           1         53020.0           53020           53020  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0       127226610           1     127226610.0       127226610       127226610  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.8        78596864        7933          9907.6            1888          128800  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   21.2        21083648         768         27452.7            1504          160128  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0            7933               49.6              4.000              764.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.2      1471099978          77      19105194.5           37329       100125906  poll                                                                            \n",
      "   40.6      1102012207          77      14311846.8           11050       106522869  sem_timedwait                                                                   \n",
      "    4.2       114825393         664        172929.8            1066        18963718  ioctl                                                                           \n",
      "    0.8        22373083          90        248589.8            1260         7776154  mmap                                                                            \n",
      "    0.0          812934          77         10557.6            2569           28634  open64                                                                          \n",
      "    0.0          116295          11         10572.3            4651           19521  write                                                                           \n",
      "    0.0          115944           4         28986.0           23161           31384  pthread_create                                                                  \n",
      "    0.0          107239          23          4662.6            1434           18193  fopen                                                                           \n",
      "    0.0           91142           3         30380.7           25372           38481  fgets                                                                           \n",
      "    0.0           49930          13          3840.8            1510            6318  munmap                                                                          \n",
      "    0.0           45888          33          1390.5            1019            4855  fcntl                                                                           \n",
      "    0.0           31678           5          6335.6            2887            9020  open                                                                            \n",
      "    0.0           30190          13          2322.3            1115            5057  read                                                                            \n",
      "    0.0           28213          16          1763.3            1053            3760  fclose                                                                          \n",
      "    0.0           18481           3          6160.3            4551            9375  pipe2                                                                           \n",
      "    0.0           11984           2          5992.0            5974            6010  socket                                                                          \n",
      "    0.0            7486           2          3743.0            3651            3835  fread                                                                           \n",
      "    0.0            6725           4          1681.3            1627            1749  mprotect                                                                        \n",
      "    0.0            5747           1          5747.0            5747            5747  connect                                                                         \n",
      "    0.0            2884           1          2884.0            2884            2884  bind                                                                            \n",
      "    0.0            1594           1          1594.0            1594            1594  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-no-prefetch-report ./vector-add-no-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开远程桌面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下一个代码单元格以生成指向远程桌面的URL，您应该通过复制该URL并将其粘贴到新的浏览器选项卡中来打开该URL。 然后，按照notebook里的指示进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "var url = window.location.hostname + '/nsight/';\nelement.append(url)\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = window.location.hostname + '/nsight/';\n",
    "element.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单击 _Connect_ 按钮后，远程桌面将要求您输入密码，即`nvidia`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开远程桌面终端应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，单击位于远程桌面屏幕底部的命令行终端应用程序图标："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![terminal](images/terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要打开Nsight Systems，请从现在打开的命令行终端中输入并运行`nsight-sys`（注意没有空格）命令："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open nsight](images/open-nsight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启用使用情况报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现提示时，单击“Yes”以启用使用情况报告："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![enable usage](images/enable_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开报告文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过从Nsight Systems菜单访问 _File_-> _Open_ 来打开此报告文件，然后转到路径 `/root/Desktop/reports`并选择`vector-add-no-prefetch-report.qdrep`。 您在本实验中生成的所有报告都将位于此`root/Desktop/reports`目录中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open-report](images/open-report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 忽略警告/错误信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以关闭并忽略看到的所有警告或错误，这仅仅是我们特定的远程桌面环境产生的结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ignore errors](images/ignore-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为活动时间表腾出更多的显示空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使您的体验更好，可全屏显示性能分析结果，即关闭 _Project Explorer_ 并隐藏 _Events View_ ："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![make nice](images/make-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "屏幕现在应如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![now nice](images/now-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展CUDA统一内存的活动时间表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，展开 _CUDA_-> _Unified memory_ 和 _Context_ 时间表，并关闭 _OS运行时库_ 的时间表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open memory](images/open-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察到有很多次的内存传输操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略看一下即知，您的应用程序运行大约需要1秒钟，并且在运行`addVectorsInto`核函数期间，还有很多的UM内存活动："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memory and kernel](images/memory-and-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放大内存的时间轴，可以更清楚地看到由按需内存页面错误引起的所有小内存传输。 几个提示：\n",
    "\n",
    "1. 您可以在滚动鼠标/触控板的同时按住`Ctrl`来放大和缩小时间轴的任何一点\n",
    "2. 您可以通过以下方式放大任意部分：单击+拖动它周围的矩形，然后选择 _Zoom in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是放大查看许多小的内存传输的示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![many transfers](images/many-transfers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 用Nsight Systems重复比较不同的代码重构后的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，您已经启动并运行了Nsight Systems，并且可以轻松地在活动时间表里移动位置。接下来，您将对您使用已经熟悉的技术进行了迭代改进的程序进行性能分析。 每做一次性能分析，活动时间表都会对下一步如何修改代码提供有用的信息。 这样做还将使您更深地理解各种CUDA编程技术如何影响应用程序的性能。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：比较预取与不预取的活动时间表\n",
    "\n",
    "[`01-vector-add-prefetch-solution.cu`](01-vector-add-prefetch-solution.cu)对上面的向量加法程序进行重构，以便它的`addVectorsInto`核函数所需的3个向量在核函数被调用之前就被异步预取到活动的GPU设备上（使用[`cudaMemPrefetchAsync`](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42)）。 请打开源代码，并确定这些更改在应用程序中的何处进行的。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印出来的成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-prefetch 01-vector-add/solutions/01-vector-add-prefetch-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t2135 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 2095 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.1       282654880           3      94218293.3           39722       282478282  cudaMallocManaged                                                               \n",
      "   16.5        61263747           1      61263747.0        61263747        61263747  cudaDeviceSynchronize                                                           \n",
      "    5.1        19125467           3       6375155.7         5297730         8140815  cudaFree                                                                        \n",
      "    2.3         8485012           3       2828337.3            6391         8350307  cudaMemPrefetchAsync                                                            \n",
      "    0.0           40467           1         40467.0           40467           40467  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0         1694779           1       1694779.0         1694779         1694779  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   75.7        65892128         192        343188.2          339872          356160  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   24.3        21164192         768         27557.5            1632          165440  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0             192             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.3      1273943159          70      18199188.0           19198       100129692  poll                                                                            \n",
      "   40.6       989491112          60      16491518.5           13239       103158845  sem_timedwait                                                                   \n",
      "    5.5       135043440         663        203685.4            1023        22629674  ioctl                                                                           \n",
      "    0.9        21447225          91        235683.8            1344         8054683  mmap                                                                            \n",
      "    0.6        14374790           3       4791596.7           41026        14215763  sem_wait                                                                        \n",
      "    0.0          699080          77          9079.0            4024           21672  open64                                                                          \n",
      "    0.0          196148          13         15088.3            4380           39394  write                                                                           \n",
      "    0.0          187251           5         37450.2           23690           48331  pthread_create                                                                  \n",
      "    0.0          125538          23          5458.2            1407           28876  fopen                                                                           \n",
      "    0.0           83291           3         27763.7           22168           36693  fgets                                                                           \n",
      "    0.0           61512          15          4100.8            1284            8602  munmap                                                                          \n",
      "    0.0           38500          15          2566.7            1164            5685  read                                                                            \n",
      "    0.0           34686           5          6937.2            4152           10909  open                                                                            \n",
      "    0.0           31894          16          1993.4            1072            6321  fclose                                                                          \n",
      "    0.0           23437           5          4687.4            1691           14746  mprotect                                                                        \n",
      "    0.0           17270           9          1918.9            1036            6810  fcntl                                                                           \n",
      "    0.0           14403           3          4801.0            4239            5884  pipe2                                                                           \n",
      "    0.0           12606           2          6303.0            5282            7324  socket                                                                          \n",
      "    0.0           11410           2          5705.0            4771            6639  fread                                                                           \n",
      "    0.0            5997           1          5997.0            5997            5997  connect                                                                         \n",
      "    0.0            2689           1          2689.0            2689            2689  bind                                                                            \n",
      "    0.0            1535           1          1535.0            1535            1535  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-prefetch-report ./vector-add-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开报告，让先前的报告保持打开的状态以进行比较。\n",
    "\n",
    "- 在添加异步预取之前，执行时间与`addVectorsInto`核函数的执行时间相比如何？\n",
    "- 在活动时间表的 *CUDA API* 部分中找到`cudaMemPrefetchAsync`。\n",
    "- 内存传输有什么变化？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：使用核函数进行向量初始化并分析其性能\n",
    "\n",
    "在向量加法程序的先前迭代中，向量数据是在CPU上初始化的，因此需要在addVectorsInto核函数对其进行操作之前将其迁移到GPU上。\n",
    "\n",
    "该应用程序的下一个迭代[01-init-kernel-solution.cu](01-init-kernel-solution.cu)已重构为在GPU上并行处理初始化数据。\n",
    "\n",
    "由于初始化现是在GPU上进行的，因此预取要在初始化之前完成的，而不是在向量加法之前完成的。 查看源代码以识别在何处进行了这些更改。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-kernel 02-init-kernel/solutions/01-init-kernel-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-kernel-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-kernel\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-kernel-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1881 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-kernel-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-kernel-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1844 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-kernel-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   89.5       225473302           3      75157767.3           21715       225389326  cudaMallocManaged                                                               \n",
      "    7.5        19010146           3       6336715.3         1151832        16516731  cudaFree                                                                        \n",
      "    2.6         6600450           1       6600450.0         6600450         6600450  cudaDeviceSynchronize                                                           \n",
      "    0.4          886323           3        295441.0           11523          789086  cudaMemPrefetchAsync                                                            \n",
      "    0.0           78597           4         19649.3            9057           47344  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.0         1856510           3        618836.7          613290          623530  initWith                                                                        \n",
      "   48.0         1713147           1       1713147.0         1713147         1713147  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21161760         768         27554.4            1632          168480  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   47.5       592000113          36      16444447.6           22050       100129083  poll                                                                            \n",
      "   42.8       533800971          33      16175787.0           10766       100067115  sem_timedwait                                                                   \n",
      "    7.9        98761840         661        149412.8            1019        15730976  ioctl                                                                           \n",
      "    1.7        20955479          91        230280.0            1347        16440076  mmap                                                                            \n",
      "    0.1          651217          77          8457.4            2755           22470  open64                                                                          \n",
      "    0.0          160198           5         32039.6           24059           39861  pthread_create                                                                  \n",
      "    0.0          109349          23          4754.3            1338           19757  fopen                                                                           \n",
      "    0.0          100519          12          8376.6            4482           14213  write                                                                           \n",
      "    0.0          100052           2         50026.0           26317           73735  sem_wait                                                                        \n",
      "    0.0           84188           3         28062.7           21977           37718  fgets                                                                           \n",
      "    0.0           47640          14          3402.9            1490            6636  munmap                                                                          \n",
      "    0.0           38000           2         19000.0            6833           31167  socket                                                                          \n",
      "    0.0           35146          14          2510.4            1125            5251  read                                                                            \n",
      "    0.0           32227           5          6445.4            2990            9091  open                                                                            \n",
      "    0.0           28003          16          1750.2            1047            3737  fclose                                                                          \n",
      "    0.0           15528          10          1552.8            1005            4479  fcntl                                                                           \n",
      "    0.0           14004           3          4668.0            4322            5229  pipe2                                                                           \n",
      "    0.0            9836           5          1967.2            1582            2541  mprotect                                                                        \n",
      "    0.0            8781           3          2927.0            1607            3969  fread                                                                           \n",
      "    0.0            6006           1          6006.0            6006            6006  connect                                                                         \n",
      "    0.0            2850           1          2850.0            2850            2850  bind                                                                            \n",
      "    0.0            1437           1          1437.0            1437            1437  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-kernel-report ./init-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开新的报告文件，然后执行以下操作：\n",
    "\n",
    "- 将应用程序和`addVectorsInto`运行时间与应用程序的先前版本进行比较，有什么变化？\n",
    "- 查看时间表中的 *Kernels* 部分。 这两个核函数（`addVectorsInto`和初始化核函数）中的哪一个占用了GPU的大部分时间？\n",
    "- 您的应用程序包含以下哪项？\n",
    "   - 数据迁移（HtoD）\n",
    "   - 数据迁移（DtoH）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：使用异步预将数据取回主机，并分析其性能\n",
    "\n",
    "目前，向量加法应用程序是在主机上验证向量加法核函数的结果，所以在该应用程序的下一个重构[01-prefetch-check-solution.cu](01-prefetch-check-solution.cu)中，我们用异步预取将数据取回主机后再进行验证。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印出来的成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-host 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/prefetch-to-host-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./prefetch-to-host\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u0000Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/prefetch-to-host-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1157 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/prefetch-to-host-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/prefetch-to-host-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1121 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/prefetch-to-host-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   85.8       227730532           3      75910177.3           22142       227636960  cudaMallocManaged                                                               \n",
      "    8.3        22025276           4       5506319.0           12390        21164156  cudaMemPrefetchAsync                                                            \n",
      "    3.5         9360188           3       3120062.7          811560         7553470  cudaFree                                                                        \n",
      "    2.4         6290375           1       6290375.0         6290375         6290375  cudaDeviceSynchronize                                                           \n",
      "    0.0           82913           4         20728.2           10336           48535  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.3         1875264           3        625088.0          621066          628939  initWith                                                                        \n",
      "   47.7         1713245           1       1713245.0         1713245         1713245  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20399712          64        318745.5          311232          329632  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.1       440747134          25      17629885.4           14746       103551185  sem_timedwait                                                                   \n",
      "   42.5       424264753          28      15152312.6           22476       100128264  poll                                                                            \n",
      "   12.1       121363564         659        184163.2            1053        21050129  ioctl                                                                           \n",
      "    1.1        11326159          91        124463.3            1369         7479917  mmap                                                                            \n",
      "    0.1          620915          77          8063.8            2568           18762  open64                                                                          \n",
      "    0.0          162405           5         32481.0           23496           38597  pthread_create                                                                  \n",
      "    0.0          124217          23          5400.7            1540           19455  fopen                                                                           \n",
      "    0.0          103727          12          8643.9            4437           14917  write                                                                           \n",
      "    0.0           95772           2         47886.0           24342           71430  sem_wait                                                                        \n",
      "    0.0           81567           3         27189.0           19517           37246  fgets                                                                           \n",
      "    0.0           45624          14          3258.9            1408            5539  munmap                                                                          \n",
      "    0.0           33133          14          2366.6            1171            3534  read                                                                            \n",
      "    0.0           33125           5          6625.0            2777           10211  open                                                                            \n",
      "    0.0           27847          16          1740.4            1011            3477  fclose                                                                          \n",
      "    0.0           13576           3          4525.3            3938            5317  pipe2                                                                           \n",
      "    0.0           12689           2          6344.5            4920            7769  socket                                                                          \n",
      "    0.0           12648           8          1581.0            1000            4611  fcntl                                                                           \n",
      "    0.0            9354           3          3118.0            1612            4337  fread                                                                           \n",
      "    0.0            8954           5          1790.8            1602            2007  mprotect                                                                        \n",
      "    0.0            5760           1          5760.0            5760            5760  connect                                                                         \n",
      "    0.0            2251           1          2251.0            2251            2251  bind                                                                            \n",
      "    0.0            1684           1          1684.0            1684            1684  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o prefetch-to-host-report ./prefetch-to-host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开此报告文件，然后执行以下操作：\n",
    "\n",
    "- 使用时间表里的 *Unified Memory*部分，对比向CPU添加预取之前和之后的*Data Migration (DtoH)*事件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 并发CUDA流\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您现在将学习一个新概念 **CUDA Streams**。 在对它们进行介绍之后，您将返回使用Nsight Systems来更好地评估它们对应用程序性能的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下幻灯片以可视化的方式概要地介绍了即将学习的内容。请在进入以下更详细的内容之前浏览每一页幻灯片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 CUDA 编程中，**流**是由按顺序执行的一系列命令构成。在 CUDA 应用程序中，核函数的执行以及一些内存传输均在 CUDA 流中进行。不过直至此时，您仍未直接与 CUDA 流打交道；但实际上您的 CUDA 代码已在名为*默认流*的流中执行了其核函数。\n",
    "\n",
    "除默认流以外，CUDA 程序员还可创建并使用非默认 CUDA 流，此举可支持执行多个操作，例如在不同的流中并发执行多个核函数。多流的使用可以为您的加速应用程序带来另外一个层次的并行，并能提供更多应用程序的优化机会。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制CUDA流行为的规则\n",
    "\n",
    "为有效利用 CUDA 流，您应了解有关 CUDA 流行为的以下几项规则：\n",
    "\n",
    "- 给定流中的所有操作会按序执行。\n",
    "- 就不同非默认流中的操作而言，无法保证其会按彼此之间的任何特定顺序执行。\n",
    "- 默认流具有阻断能力，即，它会等待其它已在运行的所有流完成当前操作之后才运行，但在其自身运行完毕之前亦会阻碍其它流的运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建，使用和销毁非默认CUDA流\n",
    "\n",
    "以下代码段演示了如何创建，利用和销毁非默认CUDA流。您会注意到，要在非默认CUDA流中启动CUDA核函数，必须将流作为执行配置的第4个可选参数传递给该核函数。到目前为止，您仅利用了执行配置的前两个参数：\n",
    "\n",
    "``` C++\n",
    "cudaStream_t stream;   // CUDA流的类型为 `cudaStream_t`\n",
    "cudaStreamCreate(&stream); // 注意，必须将一个指针传递给 `cudaCreateStream`\n",
    "\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>();   // `stream` 作为第4个EC参数传递\n",
    "\n",
    "cudaStreamDestroy(stream); // 注意，将值（而不是指针）传递给 `cudaDestroyStream`\n",
    "```\n",
    "\n",
    "但值得一提的是，执行配置的第3个可选参数超出了本实验的范围。此参数允许程序员提供**共享内存**（当前将不涉及的高级主题）中为每个内核启动动态分配的字节数。每个块分配给共享内存的默认字节数为“0”，在本练习的其余部分中，您将传递“ 0”作为该值，以便展示我们感兴趣的第4个参数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：预测默认流行为\n",
    "\n",
    "[01-print-numbers](01-print-numbers.cu) 应用程序带有一个非常简单的 `printNumber` 核函数，可用于接受及打印整数。仅在单个线程块内使用单线程执行该核函数，但使用“for 循环”可执行 5 次，并且每次启动时都会传递“for 循环”的迭代次数。\n",
    "\n",
    "使用下方的代码执行线程块编译和运行 [01-print-numbers](01-print-numbers.cu) 应用程序。您应能看到打印的数字 `0` 至 `4`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既已了解核函数在默认情况下会在默认流中执行，那么据您预计，`print-numbers` 程序的 5 次启动将会顺次执行还是会并行执行？您应能提及默认流的两个功能来支持您的回答。在下面的单元格中创建一个报告文件，然后在Nsight Systems中将其打开以确认您的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./print-numbers\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1018 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 984 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       201943719           5      40388743.8            5050       201918112  cudaLaunchKernel                                                                \n",
      "    0.1          270116           1        270116.0          270116          270116  cudaDeviceSynchronize                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          284149           5         56829.8           54935           63382  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.9       233274819          14      16662487.1           13460       100060408  sem_timedwait                                                                   \n",
      "   37.9       201154235          14      14368159.6           21630       100128984  poll                                                                            \n",
      "   17.5        93187353         643        144925.9            1073        15586397  ioctl                                                                           \n",
      "    0.4         2259927          82         27560.1            1228          702752  mmap                                                                            \n",
      "    0.1          644402          77          8368.9            2597           20075  open64                                                                          \n",
      "    0.0          119165           4         29791.3           22946           34951  pthread_create                                                                  \n",
      "    0.0          111923          23          4866.2            1532           20509  fopen                                                                           \n",
      "    0.0          100429          12          8369.1            4307           13743  write                                                                           \n",
      "    0.0           84859           3         28286.3           22863           37880  fgets                                                                           \n",
      "    0.0           34210          14          2443.6            1255            4356  read                                                                            \n",
      "    0.0           30621           5          6124.2            2848            9061  open                                                                            \n",
      "    0.0           29321          16          1832.6            1028            3899  fclose                                                                          \n",
      "    0.0           21197           7          3028.1            1681            4493  munmap                                                                          \n",
      "    0.0           13705           2          6852.5            5832            7873  socket                                                                          \n",
      "    0.0           13072           3          4357.3            3570            4880  pipe2                                                                           \n",
      "    0.0            9963           2          4981.5            4937            5026  fread                                                                           \n",
      "    0.0            8470           4          2117.5            1003            4913  fcntl                                                                           \n",
      "    0.0            7001           4          1750.3            1555            2102  mprotect                                                                        \n",
      "    0.0            6263           1          6263.0            6263            6263  connect                                                                         \n",
      "    0.0            2624           1          2624.0            2624            2624  fflush                                                                          \n",
      "    0.0            2321           1          2321.0            2321            2321  bind                                                                            \n",
      "    0.0            1379           1          1379.0            1379            1379  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-report ./print-numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：实现并发CUDA流\n",
    "\n",
    "由于核函数的所有 5 次启动均在同一个流中发生，因此看到 5 个核函数顺次执行也就不足为奇。此外，也可以这么说，由于默认流具有阻断作用，所以核函数都会在完成本次启动之后才启动下一次，而事实也是如此。\n",
    "\n",
    "请重构 [01-print-numbers](01-print-numbers.cu) 应用程序，以便核函数的每次启动都在自身非默认流中进行。若已不再需要所创建的流，请务必予以销毁。请使用下方的代码执行单元编译和运行经重构的代码。您应仍能看到打印的数字 `0` 至 `4`，不过这些数字不一定会按升序排列。如您遇到问题，请参阅 [解决方案](01-print-numbers-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers-in-streams 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您既已为核函数的 5 次启动使用 5 个不同的非默认流，您预计它们会顺次执行还是会并行执行？除目前对流的了解之外，您还需考虑 `printNumber` 核函数的简单程度，也就是说，即使您预测并行运行，核函数的完成速度是否仍会允许完全重叠？\n",
    "\n",
    "进行假设后，在Nsight Systems中打开一个新的报告文件以查看其实际行为。 您应该注意到，现在，_CUDA_ 部分中为您创建的每个非默认流提供了其他行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = print-numbers-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1044 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1005 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       230678664           5      46135732.8            2349       230667553  cudaStreamCreate                                                                \n",
      "    0.0           89980           5         17996.0            7068           57475  cudaLaunchKernel                                                                \n",
      "    0.0           69084           1         69084.0           69084           69084  cudaDeviceSynchronize                                                           \n",
      "    0.0           29721           5          5944.2            3680           13418  cudaStreamDestroy                                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          304259           5         60851.8           56058           65850  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   45.0       274152667           9      30461407.4           12647       101217411  sem_timedwait                                                                   \n",
      "   37.7       229952639          14      16425188.5            1893        96114730  poll                                                                            \n",
      "   16.6       100983012         649        155597.9            1011        15761754  ioctl                                                                           \n",
      "    0.5         2870001          82         35000.0            1215          726567  mmap                                                                            \n",
      "    0.1          700868          77          9102.2            2599           23592  open64                                                                          \n",
      "    0.0          293126          23         12744.6            1608          204403  fopen                                                                           \n",
      "    0.0          199463          12         16621.9            4308           41248  write                                                                           \n",
      "    0.0          123603           4         30900.8           24377           41068  pthread_create                                                                  \n",
      "    0.0           82832           3         27610.7           21120           37482  fgets                                                                           \n",
      "    0.0           38103           5          7620.6            2696           10910  open                                                                            \n",
      "    0.0           27937          14          1995.5            1078            2979  read                                                                            \n",
      "    0.0           26680          15          1778.7            1000            4104  fclose                                                                          \n",
      "    0.0           22514           7          3216.3            1856            4803  munmap                                                                          \n",
      "    0.0           20001          15          1333.4            1000            3657  fcntl                                                                           \n",
      "    0.0           13167           3          4389.0            3295            5095  pipe2                                                                           \n",
      "    0.0           13062           2          6531.0            5479            7583  socket                                                                          \n",
      "    0.0            7428           4          1857.0            1672            2080  mprotect                                                                        \n",
      "    0.0            6698           2          3349.0            2609            4089  fread                                                                           \n",
      "    0.0            6428           1          6428.0            6428            6428  connect                                                                         \n",
      "    0.0            3518           1          3518.0            3518            3518  fflush                                                                          \n",
      "    0.0            2527           1          2527.0            2527            2527  bind                                                                            \n",
      "    0.0            1560           1          1560.0            1560            1560  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-in-streams-report print-numbers-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stream print](images/streams-print.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：将流用于并行进行数据初始化的核函数\n",
    "\n",
    "您一直使用的向量加法应用程序 [01-prefetch-check-solution.cu](01-prefetch-check-solution.cu) 目前启动 3 次初始化核函数，即：为 `vectorAdd` 核函数需要初始化的 3 个向量分别启动一次。重构该应用程序，以便在其各自的非默认流中启动全部 3 个初始化核函数。在使用下方的代码执行单元编译及运行时，您应仍能看到打印的成功消息。如您遇到问题，请参阅 [解决方案](01-stream-init-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-in-streams 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开一个报告，以确认您的3个初始化内核启动正在它们自己的非默认流中运行，并且存在一定程度的并发重叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u0000Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1165 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1127 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   85.8       222344380           3      74114793.3           20698       222258253  cudaMallocManaged                                                               \n",
      "    8.5        22008028           4       5502007.0           12063        21105077  cudaMemPrefetchAsync                                                            \n",
      "    3.3         8671761           3       2890587.0          850101         6863700  cudaFree                                                                        \n",
      "    2.3         6057487           1       6057487.0         6057487         6057487  cudaDeviceSynchronize                                                           \n",
      "    0.0           98078           4         24519.5           12207           55510  cudaLaunchKernel                                                                \n",
      "    0.0           30780           3         10260.0            3572           23000  cudaStreamCreate                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.3         2030990           3        676996.7          644220          701881  initWith                                                                        \n",
      "   45.7         1710144           1       1710144.0         1710144         1710144  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20344384          64        317881.0          311104          319456  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.3       413091002          24      17212125.1           10833       100064060  sem_timedwait                                                                   \n",
      "   43.2       412035536          27      15260575.4           24495       100139523  poll                                                                            \n",
      "   12.2       116280607         664        175121.4            1016        21056000  ioctl                                                                           \n",
      "    1.1        10707152          91        117661.0            1394         6795773  mmap                                                                            \n",
      "    0.1          631342          77          8199.2            2588           19459  open64                                                                          \n",
      "    0.0          163401           5         32680.2           24784           38207  pthread_create                                                                  \n",
      "    0.0          110468          23          4803.0            1414           20211  fopen                                                                           \n",
      "    0.0          108665           2         54332.5           30703           77962  sem_wait                                                                        \n",
      "    0.0           98741          12          8228.4            4523           13067  write                                                                           \n",
      "    0.0           85067           3         28355.7           22918           37754  fgets                                                                           \n",
      "    0.0           49143          15          3276.2            1522            5888  munmap                                                                          \n",
      "    0.0           35738          14          2552.7            1120            5309  read                                                                            \n",
      "    0.0           33137           5          6627.4            3617            8973  open                                                                            \n",
      "    0.0           28422          16          1776.4            1016            3890  fclose                                                                          \n",
      "    0.0           13152           3          4384.0            3794            5243  pipe2                                                                           \n",
      "    0.0           13132           2          6566.0            5642            7490  socket                                                                          \n",
      "    0.0           11692           7          1670.3            1003            4519  fcntl                                                                           \n",
      "    0.0            8992           3          2997.3            1608            3941  fread                                                                           \n",
      "    0.0            8728           5          1745.6            1539            1909  mprotect                                                                        \n",
      "    0.0            6430           1          6430.0            6430            6430  connect                                                                         \n",
      "    0.0            2384           1          2384.0            2384            2384  bind                                                                            \n",
      "    0.0            1506           1          1506.0            1506            1506  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-in-streams-report ./init-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 总结\n",
    "\n",
    "在本实验中，您学习了：\n",
    "\n",
    "- 使用 **Nsight Systems** 直观地描述GPU加速的CUDA应用程序的活动时间表。\n",
    "- 使用Nsight Systems识别和利用GPU加速的CUDA应用程序中的优化机会。\n",
    "- 利用CUDA流在被加速的应用程序中并发执行核函数。\n",
    "\n",
    "此时，您拥有大量的基本工具和技术，可用于加速原本只能在CPU上的应用程序，然后不断优化那些被加速的应用程序。 在最后的练习中，您将有机会应用学到的所有知识来加速[n-body](https://en.wikipedia.org/wiki/N-body_problem)模拟器，以预测受引力互相影响的一组物体的集合。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 最后的练习：加速和优化N体模拟器\n",
    "\n",
    "[n-body](https://en.wikipedia.org/wiki/N-body_problem) 模拟器可以预测通过引力相互作用的一组物体的个体运动。[01-nbody.cu](nbody-unoptimize.cu) 包含一个简单而有效的 n-body 模拟器，适合用于在三维空间移动的物体。我们可通过向该应用程序传递一个命令行参数以影响系统中的物体数量。[解决方案丑陋版](01-nbody.cu)\n",
    "\n",
    "以目前的仅用CPU的情况下，此应用程序大约需要5秒钟才能运行4096个物体，需要**20分钟**才能运行65536个物体。您的任务是用GPU加速程序，同时保持仿真的正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 做此练习时可考虑的注意事项\n",
    "\n",
    "在开始任务之前，请注意以下事项：\n",
    "\n",
    "- 在第一次重构中，请格外注意应用程序的逻辑部分（尤其是 `bodyForce` 函数）能够并且应该基本保持不变：要侧重于尽可能轻松地加速应用程序。\n",
    "- 代码库包含 `main` 函数内的“for 循环”，用于将 `bodyForce` 函数计算的物体间的引力集成到系统中每个物体的所在位置。该集成不仅需在 `bodyForce` 函数运行后进行，并且需在下一次调用 `bodyForce` 函数之前完成。在选择并行化的处理方式和程序位置时，请牢记这一点。\n",
    "- 使用基于性能分析驱动和迭代的方法。\n",
    "- 无需为代码添加错误处理，但其可能有助您确保代码的顺利运行。\n",
    "\n",
    "快去开心地执行任务吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此单元格来编译nbody模拟器。尽管它最初只是一个仅用于CPU的应用程序，但却可以准确地模拟物体的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvcc -std=c++11 -o nbody 09-nbody/01-nbody.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强烈建议您使用性能分析器来协助您的工作。执行以下单元格以生成性能分析报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/nbody-report\n",
      "\tforce-overwrite = true\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./nbody\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u000011.967 Billion Interactions / second\tGenerating the /dli/task/nbody-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1288 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/nbody-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/nbody-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1253 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/nbody-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   96.3       366591605           1     366591605.0       366591605       366591605  cudaMallocManaged                                                               \n",
      "    2.8        10531131          20        526556.6            3265         1142111  cudaDeviceSynchronize                                                           \n",
      "    0.8         2934445          10        293444.5          151959          812455  cudaMemcpyAsync                                                                 \n",
      "    0.1          304372           1        304372.0          304372          304372  cudaMemPrefetchAsync                                                            \n",
      "    0.0          128969           2         64484.5            2753          126216  cudaFree                                                                        \n",
      "    0.0          128221          10         12822.1            8674           40462  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        10456754          10       1045675.4          869110         1138197  bodyForce                                                                       \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   36.3          235072          45          5223.8            2176           13632  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   35.8          231360          80          2892.0            1440           10240  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "   27.9          180542          10         18054.2           17855           19424  [CUDA memcpy DtoD]                                                              \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "           1056.0              80               13.2              4.000               60.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "            992.0              45               22.0              4.000               72.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "            960.0              10               96.0             96.000               96.0  [CUDA memcpy DtoD]                                                              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   49.2       463891713           8      57986464.1         2065400       100073601  sem_timedwait                                                                   \n",
      "   36.2       341603562          18      18977975.7           31114       100140756  poll                                                                            \n",
      "   13.8       130361021         654        199328.8            1007        16675506  ioctl                                                                           \n",
      "    0.4         3881879          84         46212.8            1237          779336  mmap                                                                            \n",
      "    0.1         1118534          77         14526.4            3553           38237  open64                                                                          \n",
      "    0.1          994710          11         90428.2           15708          613908  write                                                                           \n",
      "    0.0          189794          25          7591.8            1752           44069  fopen                                                                           \n",
      "    0.0          131436           4         32859.0           25296           42242  pthread_create                                                                  \n",
      "    0.0          105122           3         35040.7           24523           52009  fgets                                                                           \n",
      "    0.0           83963          56          1499.3            1004            5761  fcntl                                                                           \n",
      "    0.0           44545          10          4454.5            1496            8577  munmap                                                                          \n",
      "    0.0           42468          17          2498.1            1294            6610  fclose                                                                          \n",
      "    0.0           40115           5          8023.0            4137           11207  open                                                                            \n",
      "    0.0           38232          13          2940.9            1395            4462  read                                                                            \n",
      "    0.0           19219           3          6406.3            3155            8643  pipe2                                                                           \n",
      "    0.0           19047           3          6349.0            3120           12124  fwrite                                                                          \n",
      "    0.0           18885           2          9442.5            9306            9579  fopen64                                                                         \n",
      "    0.0           16134           2          8067.0            7074            9060  socket                                                                          \n",
      "    0.0            8356           2          4178.0            3651            4705  fread                                                                           \n",
      "    0.0            7493           1          7493.0            7493            7493  connect                                                                         \n",
      "    0.0            7207           4          1801.8            1704            1994  mprotect                                                                        \n",
      "    0.0            5202           1          5202.0            5202            5202  fflush                                                                          \n",
      "    0.0            2895           1          2895.0            2895            2895  bind                                                                            \n",
      "    0.0            2156           1          2156.0            2156            2156  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true --force-overwrite=true -o nbody-report ./nbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们导入一个函数，该函数将针对各种数量的物体运行您的`nbody`模拟器，以检查其性能和准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessment import run_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下单元格以运行并评估`nbody`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用4096个物体运行n-体模拟器\n",
      "----------------------------------------\n",
      "\n",
      "此应用程序应该运行得快于0.9秒。\n",
      "您的应用程序运行了: 0.1718秒\n",
      "您的应用程序运行速度是  14.507 Billion Interactions / second\n",
      "您的结果是正确的。\n",
      "\n",
      "使用65536个物体运行n-体模拟器\n",
      "----------------------------------------\n",
      "\n",
      "此应用程序应该运行得快于1.3秒。\n",
      "您的应用程序运行了: 0.4860秒\n",
      "您的应用程序运行速度是  124.875 Billion Interactions / second\n",
      "您的结果是正确的。\n",
      "\n",
      "祝贺您！您通过了评估！\n",
      "请参阅下面的说明来生成证书，并查看您是否可以进一步加速模拟器！\n"
     ]
    }
   ],
   "source": [
    "run_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成证书"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您通过了评估，请返回课程页面（如下所示）并单击\"ASSESS TASK\"（评估任务）按钮，这将为您生成该课程的证书。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run_assessment](./images/run_assessment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶内容\n",
    "\n",
    "以下章节专为时间富余和有意深究的学习者而设，其中将介绍更多中级技术，其中会涉及部分手动内存管理，以及使用非默认流重叠执行核函数和内存拷贝。\n",
    "\n",
    "在了解以下所列的各项技术后，您可尝试运用这些技术进一步优化 n-body 模拟器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 手动内存分配和复制\n",
    "\n",
    "尽管 `cudaMallocManaged` 和 `cudaMemPrefetchAsync` 函数性能出众并能大幅简化内存迁移，但有时也有必要使用更多手动内存分配方法。这在已知只需在设备或主机上访问数据时尤其如此，并且因免于进行自动按需迁移而能够收回数据迁移成本。\n",
    "\n",
    "此外，通过手动内存管理，您可以使用非默认流同时开展数据传输与计算工作。在本节中，您将学习一些基本的手动内存分配和拷贝技术，之后会延伸应用这些技术以同时开展数据拷贝与计算工作。\n",
    "\n",
    "以下是一些用于手动内存管理的 CUDA 命令：\n",
    "\n",
    "- `cudaMalloc` 命令将直接为处于活动状态的 GPU 分配内存。这可防止出现所有 GPU 分页错误，而代价是主机代码将无法访问该命令返回的指针。\n",
    "- `cudaMallocHost` 命令将直接为 CPU 分配内存。该命令可“固定”内存(pinned memory)或“页锁定”内存(page-locked memory)，此举允许将内存异步拷贝至 GPU 或从 GPU 异步拷贝至内存。固定内存过多则会干扰 CPU 性能，因此请勿无端使用该命令。释放固定内存时应使用 `cudaFreeHost` 命令。\n",
    "- 无论是从主机到设备还是从设备到主机，`cudaMemcpy` 命令均可拷贝（而非传输）内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动内存管理示例\n",
    "\n",
    "以下是一段演示使用上述 CUDA API 调用的代码。\n",
    "\n",
    "```cpp\n",
    "int *host_a, *device_a;        // Define host-specific and device-specific arrays.\n",
    "cudaMalloc(&device_a, size);   // `device_a` is immediately available on the GPU.\n",
    "cudaMallocHost(&host_a, size); // `host_a` is immediately available on CPU, and is page-locked, or pinned.\n",
    "\n",
    "initializeOnHost(host_a, N);   // No CPU page faulting since memory is already allocated on the host.\n",
    "\n",
    "// `cudaMemcpy` takes the destination, source, size, and a CUDA-provided variable for the direction of the copy.\n",
    "cudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "kernel<<<blocks, threads, 0, someStream>>>(device_a, N);\n",
    "\n",
    "// `cudaMemcpy` can also copy data from device to host.\n",
    "cudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "verifyOnHost(host_a, N);\n",
    "\n",
    "cudaFree(device_a);\n",
    "cudaFreeHost(host_a);          // Free pinned memory like this.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：手动分配主机和设备内存\n",
    "\n",
    "向量加法应用程序[01-stream-init-solution](01-stream-init-solution.cu)的最新迭代使用 `cudaMallocManaged` 命令分配受管理的内存，该内存首先由初始化核函数在设备上使用，然后由向量加法核函数在设备上使用，然后再由主机所使用以对加法结果做验证。此时内存数据的传输是自动进行的。这是种很明智的方法，但我们也值得尝试一些手动内存分配和拷贝方法，以观察其对应用程序性能的影响。\n",
    "\n",
    "将 [01-stream-init-solution](01-stream-init-solution.cu) 应用程序重构为**不**使用 `cudaMallocManaged` 命令。为此，您需要执行以下操作：\n",
    "\n",
    "- 将调用 `cudaMallocManaged` 命令替换为调用 `cudaMalloc` 命令。\n",
    "- 创建将用于在主机上验证的额外向量。这是由于使用 `cudaMalloc` 命令分配的内存在主机上不可用，因此您必须执行此操作, 使用 `cudaMallocHost` 命令分配此主机向量。\n",
    "- 在 `addVectorsInto` 核函数运行完毕后，使用 `cudaMemcpy` 命令将包含相加结果的向量复制到使用 `cudaMallocHost` 命令创建的主机向量中。\n",
    "- 使用 `cudaFreeHost` 命令释放经由 `cudaMallocHost` 命令分配的内存。\n",
    "\n",
    "如您遇到问题，请参阅 [解决方案](01-manual-malloc-solution.cu) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector-add-manual-alloc 06-stream-init/solutions/01-stream-init-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成重构后，在Nsight Systems中打开一个报告，并使用活动时间表执行以下操作：\n",
    "\n",
    "- 注意，时间表的*统一内存*部分将不复存在。\n",
    "- 比较此时间表与之前重构的时间表，并使用时间轴标尺比较当前应用程序中 `cudaMalloc` 的运行时间与先前应用程序中 `cudaMallocManaged` 的运行时间。\n",
    "- 查看当前应用程序中初始化核函数的运行开始时间如何会晚于其在上次迭代中的运行时间。检查过时间轴后，您将发现时间差在于 `cudaMallocHost` 命令所用的时间。这很清楚地表明内存传输与内存拷贝的区别。正如您当前的操作，拷贝内存时，数据将存在于系统中的 2 个不同位置。与在上次迭代中仅分配 3 个向量相比，当前分配第 4 个主机向量会产生较小的性能成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 使用流实现数据传输和代码的重叠执行\n",
    "\n",
    "以下幻灯片概要地讲解了后面将涉及的内容。请点击浏览一遍这些幻灯片，然后再继续深入了解以下章节中的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-3-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`cudaMemcpy`之外，还有`cudaMemcpyAsync`，只要固定了主机内存，它就可以从主机到设备或从设备到主机异步复制内存，这可以通过使用`cudaMallocHost`分配它来完成。\n",
    "\n",
    "与核函数的执行类似，`cudaMemcpyAsync`在默认情况下仅相对于主机是异步的。默认情况下，它在默认流中执行，因此对于GPU上发生的其他CUDA操作而言，它是阻塞操作。但是，`cudaMemcpyAsync`函数将非默认流作为可选的第5个参数。通过向其传递非默认流，可以将内存传输与其他非默认流中发生的其他CUDA操作并发。\n",
    "\n",
    "一种常见且有用的模式是结合使用固定主机内存，非默认流中的异步内存副本和非默认流中的核函数执行，以使内存传输与核函数的执行重叠。\n",
    "\n",
    "在以下示例中，我们并非在等待整个内存拷贝完成之后再开始运行核函数，而是拷贝并处理所需的数据段，并让每个拷贝/处理中的数据段均在各自的非默认流中运行。通过使用此技术，您可以开始处理部分数据，同时为后续段并发执行内存传输。使用此技术计算操作次数的数据段特定值和数组内的偏移位置时必须格外小心，如下所示：\n",
    "\n",
    "```cpp\n",
    "int N = 2<<24;\n",
    "int size = N * sizeof(int);\n",
    "\n",
    "int *host_array;\n",
    "int *device_array;\n",
    "\n",
    "cudaMallocHost(&host_array, size);               // Pinned host memory allocation.\n",
    "cudaMalloc(&device_array, size);                 // Allocation directly on the active GPU device.\n",
    "\n",
    "initializeData(host_array, N);                   // Assume this application needs to initialize on the host.\n",
    "\n",
    "const int numberOfSegments = 4;                  // This example demonstrates slicing the work into 4 segments.\n",
    "int segmentN = N / numberOfSegments;             // A value for a segment's worth of `N` is needed.\n",
    "size_t segmentSize = size / numberOfSegments;    // A value for a segment's worth of `size` is needed.\n",
    "\n",
    "// For each of the 4 segments...\n",
    "for (int i = 0; i < numberOfSegments; ++i)\n",
    "{\n",
    "  // Calculate the index where this particular segment should operate within the larger arrays.\n",
    "  segmentOffset = i * segmentN;\n",
    "\n",
    "  // Create a stream for this segment's worth of copy and work.\n",
    "  cudaStream_t stream;\n",
    "  cudaStreamCreate(&stream);\n",
    "  \n",
    "  // Asynchronously copy segment's worth of pinned host memory to device over non-default stream.\n",
    "  cudaMemcpyAsync(&device_array[segmentOffset],  // Take care to access correct location in array.\n",
    "                  &host_array[segmentOffset],    // Take care to access correct location in array.\n",
    "                  segmentSize,                   // Only copy a segment's worth of memory.\n",
    "                  cudaMemcpyHostToDevice,\n",
    "                  stream);                       // Provide optional argument for non-default stream.\n",
    "                  \n",
    "  // Execute segment's worth of work over same non-default stream as memory copy.\n",
    "  kernel<<<number_of_blocks, threads_per_block, 0, stream>>>(&device_array[segmentOffset], segmentN);\n",
    "  \n",
    "  // `cudaStreamDestroy` will return immediately (is non-blocking), but will not actually destroy stream until\n",
    "  // all stream operations are complete.\n",
    "  cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：核函数和内存复制回主机重叠执行\n",
    "\n",
    "向量加法应用程序 [01-manual-malloc-solution.cu](01-manual-malloc-solution.cu) 的最新迭代目前正在 GPU 上执行所有向量加法操作，完成后其会将内存拷贝回主机以进行验证。\n",
    "\n",
    "重构 [01-manual-malloc-solution.cu](01-manual-malloc-solution.cu) 应用程序，在非默认流中分4段执行矢量加法，以便可以在等待所有向量加法工作完成之前开始异步内存复制。如您遇到问题，请参阅 [解决方案](01-overlap-xfer-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector-add-manual-alloc 07-manual-malloc/solutions/01-manual-malloc-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成重构后，在Nsight Systems中打开一个报告，并使用活动时间表执行以下操作：\n",
    "\n",
    "- 记录设备到主机的内存传输开始时间是在所有核函数工作完成之前还是之后？\n",
    "- 需注意 4 个内存拷贝段本身并不重叠。即使是在单独的非默认流中，在给定方向（此处为 DtoH）上，每次也只能同时进行一个内存传输。此处获得性能提升的原因在于其能先于其他方式开始内存传输，并且不难想象：若在某个应用程序中所完成的工作量与简单的加法运算相比，并非可以几乎忽略不计，那么内存拷贝不仅开始得更早，而且还会与核函数的执行相重叠。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
